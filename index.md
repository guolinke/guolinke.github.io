---
layout: homepage
---

## About Me


## Research Interests


## News


## Publications （chronological）

- Zhu, Jinhua, Zhenyu He, Ziyao Li, Guolin Ke, and Linfeng Zhang. ‘Uni-Fold MuSSe: De Novo Protein Complex Prediction with Protein Language Models’. BioRxiv, 2023, 2023–2002.

- Yu, Yuejiang, Shuqi Lu, Zhifeng Gao, Hang Zheng, and Guolin Ke. ‘Do Deep Learning Models Really Outperform Traditional Approaches in Molecular Docking?’ ArXiv Preprint ArXiv:2302. 07134, 2023.

- Zhou, Gengmo, Zhifeng Gao, Zhewei Wei, Hang Zheng, and Guolin Ke. ‘Do Deep Learning Methods Really Perform Better in Molecular Conformation Generation?’ ArXiv Preprint ArXiv:2302. 07061, 2023.

- Lu, Shuqi, Lin Yao, Xi Chen, Hang Zheng, Di He, and Guolin Ke. ‘3D Molecular Generation via Virtual Dynamics’. ArXiv Preprint ArXiv:2302. 05847, 2023.

- Yao, Lin, Ruihan Xu, Zhifeng Gao, Guolin Ke, and Yuhang Wang. ‘Boosted Ab Initio Cryo-EM 3D Reconstruction with ACE-EM’. ArXiv Preprint ArXiv:2302. 06091, 2023.

- Zhou, Gengmo, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. ‘Uni-Mol: A Universal 3D Molecular Representation Learning Framework’. In International Conference on Learning Representations, n.d.

- Yao, Lin, Ruihan Xu, Guolin Ke, Zhifeng Gao, and Yuhang Wang. ‘ACE-EM: Boosted Ab Initio Cryo-EM 3D Reconstruction with Asymmetric Complementary Autoencoder’, n.d.

- Li, Ziyao, Xuyang Liu, Weijie Chen, Fan Shen, Hangrui Bi, Guolin Ke, and Linfeng Zhang. ‘Uni-Fold: An Open-Source Platform for Developing Protein Folding Models beyond AlphaFold’. BioRxiv, 2022, 2022–2008.

- Shi, Yu, Guolin Ke, Zhuoming Chen, Shuxin Zheng, and Tie-Yan Liu. ‘Quantized Training of Gradient Boosting Decision Trees’. ArXiv Preprint ArXiv:2207. 09682, 2022.

- Bajaj, Payal, Chenyan Xiong, Guolin Ke, Xiaodong Liu, Di He, Saurabh Tiwary, Tie-Yan Liu, Paul Bennett, Xia Song, and Jianfeng Gao. ‘Metro: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals’. ArXiv Preprint ArXiv:2204. 06644, 2022.

- Lu, Shuqi, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu, and Arnold Overwijk. ‘Less Is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder’. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2780–91, 2021.

- Luo, Shengjie, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, and Tie-Yan Liu. ‘Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding’. Advances in Neural Information Processing Systems 34 (2021): 22795–807.

- Wu, Qiyu, Chen Xing, Yatao Li, Guolin Ke, Di He, and Tie-Yan Liu. ‘Taking Notes on the Fly Helps Language Pre-Training’. In International Conference on Learning Representations, 2021.

- Ying, Chengxuan, Mingqi Yang, Shuxin Zheng, Guolin Ke, Shengjie Luo, Tianle Cai, Chenglin Wu, Yuxin Wang, Yanming Shen, and Di He. ‘First Place Solution of KDD Cup 2021 & OGB Large-Scale Challenge Graph Prediction Track’. ArXiv Preprint ArXiv:2106. 08279, 2021.

- Ying, Chengxuan, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. ‘Do Transformers Really Perform Badly for Graph Representation?’ Advances in Neural Information Processing Systems 34 (2021): 28877–88.

- Peng, Dinglan, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, and Tie-Yan Liu. ‘How Could Neural Networks Understand Programs?’ In International Conference on Machine Learning, 8476–86. PMLR, 2021.

- Ying, Chengxuan, Guolin Ke, Di He, and Tie-Yan Liu. ‘Lazyformer: Self Attention with Lazy Update’. ArXiv Preprint ArXiv:2102. 12702, 2021.

- Ke, Guolin, Di He, and Tie-Yan Liu. ‘Rethinking Positional Encoding in Language Pre-Training’. In International Conference on Learning Representations (ICLR) 2021, 2020.

- Xu, Zhenhui, Linyuan Gong, Guolin Ke, Di He, Shuxin Zheng, Liwei Wang, Jiang Bian, and Tie-Yan Liu. ‘Mc-Bert: Efficient Language Pre-Training via a Meta Controller’. ArXiv Preprint ArXiv:2006. 05744, 2020.

- Xiao, Mingqing, Shuxin Zheng, Chang Liu, Yaolong Wang, Di He, Guolin Ke, Jiang Bian, Zhouchen Lin, and Tie-Yan Liu. ‘Invertible Image Rescaling’. In Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part I 16, 126–44. Springer International Publishing, 2020.

- Ke, Guolin, Zhenhui Xu, Jia Zhang, Jiang Bian, and Tie-Yan Liu. ‘DeepGBM: A Deep Learning Framework Distilled by GBDT for Online Prediction Tasks’. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 384–94, 2019.

- Xu, Zhenhui, Guolin Ke, Jia Zhang, Jiang Bian, and Tie-Yan Liu. ‘Light Multi-Segment Activation for Model Compression’. In AAAI, 6542–49, 2020.

- Ke, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. ‘LightGBM: A Highly Efficient Gradient Boosting Decision Tree’. In Advances in Neural Information Processing Systems, 3148–56, 2017.

- Meng, Qi, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, and Tie-Yan Liu. ‘A Communication-Efficient Parallel Algorithm for Decision Tree’. In Advances in Neural Information Processing Systems, 1279–87, 2016.
